import os
import json
import hashlib
import faiss
import numpy as np
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer

# ê²½ë¡œ ì„¤ì •

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'data')              # hr_demo/data í´ë”
PDF_DIR = os.path.join(BASE_DIR, 'pdfs')
CLEAN_DIR = os.path.join(BASE_DIR, 'clean')
DB_DIR = os.path.join(BASE_DIR, 'db')


os.makedirs(CLEAN_DIR, exist_ok=True)
os.makedirs(DB_DIR, exist_ok=True)

# ì„¤ì •ê°’
CHUNK_SIZE = 500
EMBED_BATCH = 100
MODEL_NAME = 'all-MiniLM-L6-v2'
seen_hashes = set()
model = SentenceTransformer(MODEL_NAME)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ í•¨ìˆ˜

def extract_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text
    return text.strip()

def split_text(text, size=CHUNK_SIZE):
    return [text[i:i+size] for i in range(0, len(text), size)]

def is_duplicate(text):
    h = hashlib.sha256(text.encode()).hexdigest()
    if h in seen_hashes:
        return True
    seen_hashes.add(h)
    return False

def save_chunks(title, chunks):
    path = os.path.join(CLEAN_DIR, f"{title}.json")
    with open(path, 'w', encoding='utf-8') as f:
        json.dump({"chunks": chunks}, f, ensure_ascii=False)

def already_processed(title):
    path = os.path.join(CLEAN_DIR, f"{title}.json")
    return os.path.exists(path)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë²¡í„° DB

class FaissManager:
    def __init__(self, dim, save_dir, max_per_index=10000):
        self.dim = dim
        self.save_dir = save_dir
        self.max_per_index = max_per_index
        self.index = faiss.IndexFlatL2(dim)
        self.count = 0
        self.file_index = 1

    def add(self, vectors):
        self.index.add(vectors)
        self.count += len(vectors)
        if self.count >= self.max_per_index:
            self.flush()

    def flush(self):
        if self.count == 0:
            return
        path = os.path.join(self.save_dir, f"index_{self.file_index:03}.faiss")
        faiss.write_index(self.index, path)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {path} ({self.count} vectors)")
        self.index = faiss.IndexFlatL2(self.dim)
        self.count = 0
        self.file_index += 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì‹¤í–‰

def run():
    pdf_files = [f for f in os.listdir(PDF_DIR) if f.endswith('.pdf')]
    print(f"ğŸ” PDF íŒŒì¼ {len(pdf_files)}ê°œ ë°œê²¬ë¨")

    chunk_buffer = []
    manager = FaissManager(dim=384, save_dir=DB_DIR)

    for idx, file in enumerate(pdf_files):
        title = os.path.splitext(file)[0]
        if already_processed(title):
            print(f"[{idx+1}] â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼: {file}")
            continue

        path = os.path.join(PDF_DIR, file)
        try:
            text = extract_text_from_pdf(path)
            if len(text) < 200:
                print(f"[{idx+1}] âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ: {file}")
                continue

            chunks = split_text(text)
            final_chunks = [c for c in chunks if not is_duplicate(c)]
            if not final_chunks:
                print(f"[{idx+1}] âš ï¸ ì¤‘ë³µë§Œ ì¡´ì¬: {file}")
                continue

            save_chunks(title, final_chunks)
            chunk_buffer.extend(final_chunks)

            if len(chunk_buffer) >= EMBED_BATCH:
                vectors = model.encode(chunk_buffer)
                manager.add(vectors)
                chunk_buffer.clear()

            print(f"[{idx+1}] âœ… {file} - {len(final_chunks)} chunks")
        except Exception as e:
            print(f"[{idx+1}] âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {file} | {str(e)}")

    if chunk_buffer:
        vectors = model.encode(chunk_buffer)
        manager.add(vectors)
        manager.flush()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == '__main__':
    run()
